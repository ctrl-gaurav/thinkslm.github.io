<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Towards Reasoning Ability of Small Language Models - Benchmarking various SLMs on reasoning tasks">
  <meta property="og:title" content="Towards Reasoning Ability of Small Language Models" />
  <meta property="og:description"
    content="Research exploring the reasoning capabilities of Small Language Models across various benchmarks" />
  <meta property="og:url" content="https://your-website-url.com" />
  <meta property="og:image" content="static/images/banner.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="Towards Reasoning Ability of Small Language Models">
  <meta name="twitter:description"
    content="Research exploring the reasoning capabilities of Small Language Models across various benchmarks">
  <meta name="twitter:image" content="static/images/twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords"
    content="small language models, reasoning, benchmarking, AI research, GSM8K, ARC, commonsenseQA">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Towards Reasoning Ability of Small Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    .leaderboard-section {
      padding: 2rem 0;
    }

    .leaderboard-container {
      overflow-x: auto;
      margin: 2rem auto;
    }

    .highlight-cell {
      font-weight: bold;
      background-color: rgba(0, 209, 178, 0.1);
    }

    .model-family {
      font-weight: bold;
      background-color: #f5f5f5;
    }

    .header-cell {
      cursor: pointer;
    }

    #leaderboardTable thead th {
      position: relative;
      text-align: center;
    }

    .task-column {
      min-width: 100px;
    }

    .model-column {
      min-width: 180px;
    }

    .params-column {
      min-width: 80px;
    }

    .sort-icon {
      display: inline-block;
      width: 10px;
      height: 10px;
      margin-left: 5px;
      position: relative;
    }

    .sort-icon.sort-asc::after {
      content: "▲";
      font-size: 10px;
      position: absolute;
      top: -5px;
    }

    .sort-icon.sort-desc::after {
      content: "▼";
      font-size: 10px;
      position: absolute;
      top: -5px;
    }

    .leaderboard-controls {
      margin-bottom: 1rem;
      padding: 0 1rem;
    }

    .search-box {
      max-width: 400px;
      margin: 0 auto;
      display: flex;
    }

    .search-box input {
      flex-grow: 1;
      padding: 0.5rem;
      border: 1px solid #ddd;
      border-radius: 4px 0 0 4px;
    }

    .search-box button {
      padding: 0.5rem 1rem;
      background-color: #485fc7;
      color: white;
      border: 1px solid #485fc7;
      border-radius: 0 4px 4px 0;
      cursor: pointer;
    }

    .search-box button:hover {
      background-color: #364fc7;
    }
  </style>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Reasoning Ability of Small Language Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fT_GOxEAAAAJ&hl=en" target="_blank">Gaurav
                  Srivastava</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=UhJZUHEAAAAJ&hl=en" target="_blank">Shuxiang
                  Cao</a><sup>2,3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=_IVJi6UAAAAJ&hl=en" target="_blank">Xuan
                  Wang</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Department of Computer Science, Virginia Tech, Blacksburg, VA,
                USA<br>
                <sup>2</sup>Department of Physics, Clarendon Laboratory, University of Oxford, OX1 3PU, UK<br>
                <sup>3</sup>NVIDIA Corporation<br>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2502.11569" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/ctrl-gaurav" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Will be available soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.11569" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale (~100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation.
                This raises a critical question: <em>Can SLMs achieve reasoning abilities comparable to LLMs?</em>
                In this work, we systematically <strong>survey, benchmark, and analyze</strong> <strong>72</strong> SLMs from <strong>six</strong> model families across <strong>14</strong> reasoning benchmarks. For reliable evaluation, we examine <strong>four</strong> evaluation methods and compare <strong>four</strong> LLM judges against human evaluations on <strong>800</strong> data points. We repeat all experiments <strong>three</strong> times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models.
                Beyond accuracy, we also evaluate model robustness under <strong>adversarial conditions</strong> and <strong>intermediate reasoning steps</strong>.
                Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning tasks.
              
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Leaderboard section -->
  <section class="section leaderboard-section">
    <div class="container is-max-fullhd">
      <h2 class="title is-3 has-text-centered">Small Language Models Reasoning Leaderboard</h2>
      <p class="subtitle has-text-centered">Performance of small language models across 14 reasoning benchmarks and 6
        sorting tasks along with GPU memory and disk size.
        <br> The models are sorted by their overall average performance on the reasoning benchmarks.
      </p>
      </p>

      <div class="leaderboard-controls">
        <div class="search-box">
          <input type="text" id="searchInput" placeholder="Search models...">
          <button id="searchButton">Search</button>
        </div>
      </div>

      <div class="leaderboard-container">
        <table id="leaderboardTable" class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th class="model-column">Model</th>
              <th class="params-column">Params</th>
              <th class="quant-column">Quantization</th>
              <th class="resource-column">GPU Memory (GB)</th>
              <th class="resource-column">Disk Size (GB)</th>
              <th class="task-column">GSM8K (Direct I/O)</th>
              <th class="task-column">GSM8K (COT)</th>
              <th class="task-column">GSM8K (5-Shot)</th>
              <th class="task-column">GSM8K (5-Shot COT)</th>
              <th class="task-column">GSM8K (8-Shot)</th>
              <th class="task-column">ARC-Easy</th>
              <th class="task-column">ARC-Challenge</th>
              <th class="task-column">CommonsenseQA</th>
              <th class="task-column">Sort-8 (+ve)</th>
              <th class="task-column">Sort-8 (mixed)</th>
              <th class="task-column">Sort-16 (+ve)</th>
              <th class="task-column">Sort-16 (mixed)</th>
              <th class="task-column">Sort-32 (+ve)</th>
              <th class="task-column">Sort-32 (mixed)</th>
              <th class="task-column">Sorting Average</th>
              <th class="task-column">Overall Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Llama-3.2-1B</td>
              <td>1B</td>
              <td>None</td>
              <td>4.73</td>
              <td>2.4</td>
              <td>36.39</td>
              <td>38.99</td>
              <td>33.69</td>
              <td>32.73</td>
              <td>33.13</td>
              <td>67.23</td>
              <td>47.50</td>
              <td>48.38</td>
              <td>44.67</td>
              <td>1.33</td>
              <td>1.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>7.83</td>
              <td>29.67</td>
            </tr>
            <tr>
              <td>Llama-3.2-1B</td>
              <td>1B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>1.53</td>
              <td>1.9</td>
              <td>36.87</td>
              <td>39.63</td>
              <td>32.07</td>
              <td>30.58</td>
              <td>32.88</td>
              <td>67.45</td>
              <td>47.90</td>
              <td>48.10</td>
              <td>50.33</td>
              <td>1.67</td>
              <td>1.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>8.89</td>
              <td>30.17</td>
            </tr>
            <tr>
              <td>Llama-3.2-1B</td>
              <td>1B</td>
              <td>FP8</td>
              <td>2.47</td>
              <td>1.9</td>
              <td>36.42</td>
              <td>39.63</td>
              <td>31.16</td>
              <td>30.88</td>
              <td>31.87</td>
              <td>67.03</td>
              <td>48.01</td>
              <td>48.48</td>
              <td>42.67</td>
              <td>1.33</td>
              <td>0.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>7.45</td>
              <td>29.47</td>
            </tr>
            <tr>
              <td>Llama-3.2-1B</td>
              <td>1B</td>
              <td>FP8-dynamic</td>
              <td>2.47</td>
              <td>2.0</td>
              <td>36.21</td>
              <td>40.86</td>
              <td>32.93</td>
              <td>31.24</td>
              <td>32.83</td>
              <td>67.02</td>
              <td>48.69</td>
              <td>48.40</td>
              <td>45.67</td>
              <td>2.00</td>
              <td>2.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>8.39</td>
              <td>30.29</td>
            </tr>
            <tr>
              <td>Llama-3.2-3B</td>
              <td>3B</td>
              <td>None</td>
              <td>13.21</td>
              <td>6.0</td>
              <td>73.54</td>
              <td>75.18</td>
              <td>74.02</td>
              <td>72.73</td>
              <td>72.61</td>
              <td>87.84</td>
              <td>74.63</td>
              <td>69.72</td>
              <td>96.67</td>
              <td>55.33</td>
              <td>73.33</td>
              <td>17.33</td>
              <td>17.00</td>
              <td>0.00</td>
              <td>43.28</td>
              <td>67.19</td>
            </tr>
            <tr>
              <td>Llama-3.2-3B</td>
              <td>3B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>3.66</td>
              <td>4.2</td>
              <td>72.58</td>
              <td>75.23</td>
              <td>73.39</td>
              <td>73.74</td>
              <td>72.68</td>
              <td>87.22</td>
              <td>74.37</td>
              <td>69.31</td>
              <td>95.67</td>
              <td>49.67</td>
              <td>62.67</td>
              <td>17.33</td>
              <td>15.00</td>
              <td>0.00</td>
              <td>40.06</td>
              <td>66.06</td>
            </tr>
            <tr>
              <td>Llama-3.2-3B</td>
              <td>3B</td>
              <td>FP8</td>
              <td>6.44</td>
              <td>4.2</td>
              <td>74.07</td>
              <td>75.31</td>
              <td>72.91</td>
              <td>73.04</td>
              <td>71.19</td>
              <td>88.03</td>
              <td>74.03</td>
              <td>68.74</td>
              <td>96.00</td>
              <td>41.33</td>
              <td>61.67</td>
              <td>18.00</td>
              <td>19.00</td>
              <td>0.00</td>
              <td>39.33</td>
              <td>65.57</td>
            </tr>
            <tr>
              <td>Llama-3.2-3B</td>
              <td>3B</td>
              <td>FP8-dynamic</td>
              <td>6.44</td>
              <td>4.2</td>
              <td>73.49</td>
              <td>75.13</td>
              <td>72.71</td>
              <td>73.34</td>
              <td>72.05</td>
              <td>87.53</td>
              <td>73.58</td>
              <td>69.75</td>
              <td>94.00</td>
              <td>52.33</td>
              <td>60.33</td>
              <td>16.00</td>
              <td>18.00</td>
              <td>0.00</td>
              <td>40.11</td>
              <td>65.80</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>None</td>
              <td>30.65</td>
              <td>15</td>
              <td>83.45</td>
              <td>85.27</td>
              <td>83.45</td>
              <td>84.51</td>
              <td>83.50</td>
              <td>92.07</td>
              <td>79.58</td>
              <td>74.28</td>
              <td>86.00</td>
              <td>78.67</td>
              <td>74.67</td>
              <td>56.33</td>
              <td>59.67</td>
              <td>5.33</td>
              <td>60.11</td>
              <td>77.93</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>8.98</td>
              <td>8.5</td>
              <td>83.37</td>
              <td>85.27</td>
              <td>83.45</td>
              <td>84.41</td>
              <td>83.32</td>
              <td>92.33</td>
              <td>79.98</td>
              <td>73.63</td>
              <td>82.33</td>
              <td>77.00</td>
              <td>70.67</td>
              <td>58.00</td>
              <td>62.33</td>
              <td>4.67</td>
              <td>59.17</td>
              <td>77.53</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>15.94</td>
              <td>8.5</td>
              <td>83.95</td>
              <td>84.89</td>
              <td>83.78</td>
              <td>83.75</td>
              <td>83.62</td>
              <td>92.34</td>
              <td>80.32</td>
              <td>73.87</td>
              <td>86.33</td>
              <td>79.00</td>
              <td>73.67</td>
              <td>56.00</td>
              <td>65.00</td>
              <td>5.67</td>
              <td>60.95</td>
              <td>78.01</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>12.6</td>
              <td>5.4</td>
              <td>82.21</td>
              <td>83.80</td>
              <td>82.13</td>
              <td>80.74</td>
              <td>81.70</td>
              <td>90.49</td>
              <td>76.62</td>
              <td>73.57</td>
              <td>82.67</td>
              <td>66.67</td>
              <td>69.67</td>
              <td>52.00</td>
              <td>56.67</td>
              <td>6.67</td>
              <td>55.73</td>
              <td>74.38</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>FP8</td>
              <td>14.44</td>
              <td>8.5</td>
              <td>82.89</td>
              <td>84.63</td>
              <td>83.42</td>
              <td>84.94</td>
              <td>83.83</td>
              <td>92.17</td>
              <td>79.52</td>
              <td>73.93</td>
              <td>81.00</td>
              <td>81.33</td>
              <td>72.00</td>
              <td>51.67</td>
              <td>61.00</td>
              <td>6.00</td>
              <td>58.83</td>
              <td>77.15</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>8B</td>
              <td>FP8-dynamic</td>
              <td>21.09</td>
              <td>8.5</td>
              <td>83.27</td>
              <td>84.86</td>
              <td>82.97</td>
              <td>83.88</td>
              <td>84.69</td>
              <td>92.33</td>
              <td>81.00</td>
              <td>74.09</td>
              <td>81.67</td>
              <td>74.67</td>
              <td>74.33</td>
              <td>53.00</td>
              <td>65.33</td>
              <td>5.00</td>
              <td>59.00</td>
              <td>77.13</td>
            </tr>

            <!-- Llama-3.1 70B Models -->
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>None</td>
              <td>269.17</td>
              <td>132</td>
              <td>95.10</td>
              <td>95.27</td>
              <td>94.72</td>
              <td>94.44</td>
              <td>94.64</td>
              <td>98.34</td>
              <td>94.43</td>
              <td>83.73</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>99.00</td>
              <td>97.00</td>
              <td>100.00</td>
              <td>88.00</td>
              <td>97.33</td>
              <td>94.74</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>69.34</td>
              <td>68</td>
              <td>94.72</td>
              <td>95.00</td>
              <td>94.52</td>
              <td>94.62</td>
              <td>94.54</td>
              <td>98.43</td>
              <td>94.62</td>
              <td>83.92</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>99.33</td>
              <td>96.67</td>
              <td>100.00</td>
              <td>85.33</td>
              <td>96.89</td>
              <td>94.82</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>138.64</td>
              <td>68</td>
              <td>92.92</td>
              <td>94.36</td>
              <td>93.96</td>
              <td>94.39</td>
              <td>93.51</td>
              <td>97.59</td>
              <td>92.89</td>
              <td>80.04</td>
              <td>100.00</td>
              <td>99.00</td>
              <td>98.00</td>
              <td>95.00</td>
              <td>99.00</td>
              <td>85.33</td>
              <td>96.06</td>
              <td>93.14</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>107.34</td>
              <td>38</td>
              <td>95.15</td>
              <td>95.20</td>
              <td>94.82</td>
              <td>95.12</td>
              <td>94.90</td>
              <td>98.26</td>
              <td>94.51</td>
              <td>82.77</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>98.67</td>
              <td>97.00</td>
              <td>99.67</td>
              <td>76.33</td>
              <td>95.28</td>
              <td>94.09</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>FP8</td>
              <td>107.32</td>
              <td>68</td>
              <td>94.87</td>
              <td>95.40</td>
              <td>94.67</td>
              <td>94.52</td>
              <td>94.74</td>
              <td>98.36</td>
              <td>94.71</td>
              <td>83.87</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>98.67</td>
              <td>96.33</td>
              <td>100.00</td>
              <td>86.67</td>
              <td>96.95</td>
              <td>94.92</td>
            </tr>
            <tr>
              <td>Llama-3.1</td>
              <td>70B</td>
              <td>FP8-dynamic</td>
              <td>176.63</td>
              <td>68</td>
              <td>94.64</td>
              <td>95.38</td>
              <td>95.00</td>
              <td>95.10</td>
              <td>94.52</td>
              <td>98.46</td>
              <td>94.54</td>
              <td>83.70</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>98.67</td>
              <td>97.67</td>
              <td>100.00</td>
              <td>86.00</td>
              <td>97.06</td>
              <td>95.04</td>
            </tr>
            <!-- Mistral-v0.3 7B Models -->
            <tr>
              <td>Mistral-v0.3</td>
              <td>7B</td>
              <td>None</td>
              <td>27.67</td>
              <td>14</td>
              <td>54.84</td>
              <td>55.98</td>
              <td>54.76</td>
              <td>57.90</td>
              <td>54.23</td>
              <td>88.99</td>
              <td>76.82</td>
              <td>69.83</td>
              <td>60.33</td>
              <td>48.33</td>
              <td>21.33</td>
              <td>5.67</td>
              <td>2.00</td>
              <td>1.00</td>
              <td>23.11</td>
              <td>46.93</td>
            </tr>
            <tr>
              <td>Mistral-v0.3</td>
              <td>7B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>34.84</td>
              <td>7.1</td>
              <td>52.11</td>
              <td>55.60</td>
              <td>53.88</td>
              <td>55.75</td>
              <td>52.79</td>
              <td>88.65</td>
              <td>75.97</td>
              <td>70.52</td>
              <td>55.33</td>
              <td>38.67</td>
              <td>14.67</td>
              <td>5.00</td>
              <td>0.67</td>
              <td>1.00</td>
              <td>19.22</td>
              <td>44.04</td>
            </tr>
            <tr>
              <td>Mistral-v0.3</td>
              <td>7B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>14.36</td>
              <td>7.1</td>
              <td>54.26</td>
              <td>55.85</td>
              <td>54.13</td>
              <td>56.86</td>
              <td>52.82</td>
              <td>89.07</td>
              <td>76.68</td>
              <td>70.22</td>
              <td>62.00</td>
              <td>45.00</td>
              <td>24.00</td>
              <td>5.33</td>
              <td>2.00</td>
              <td>0.00</td>
              <td>23.06</td>
              <td>46.15</td>
            </tr>
            <tr>
              <td>Mistral-v0.3</td>
              <td>7B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>11.17</td>
              <td>3.9</td>
              <td>53.93</td>
              <td>56.03</td>
              <td>51.91</td>
              <td>53.47</td>
              <td>50.87</td>
              <td>88.33</td>
              <td>74.97</td>
              <td>69.83</td>
              <td>54.00</td>
              <td>25.00</td>
              <td>16.00</td>
              <td>3.00</td>
              <td>4.00</td>
              <td>0.00</td>
              <td>17.00</td>
              <td>42.95</td>
            </tr>
            <tr>
              <td>Mistral-v0.3</td>
              <td>7B</td>
              <td>FP8</td>
              <td>--</td>
              <td>--</td>
              <td>54.13</td>
              <td>54.99</td>
              <td>53.96</td>
              <td>57.67</td>
              <td>53.85</td>
              <td>88.64</td>
              <td>76.39</td>
              <td>69.48</td>
              <td>58.33</td>
              <td>45.67</td>
              <td>21.00</td>
              <td>5.33</td>
              <td>2.67</td>
              <td>0.00</td>
              <td>22.17</td>
              <td>46.09</td>
            </tr>

            <!-- Mistral-Nemo 12B Models -->
            <tr>
              <td>Mistral-Nemo</td>
              <td>12B</td>
              <td>None</td>
              <td>57.89</td>
              <td>23</td>
              <td>86.76</td>
              <td>86.08</td>
              <td>85.57</td>
              <td>84.94</td>
              <td>85.34</td>
              <td>92.79</td>
              <td>83.70</td>
              <td>72.78</td>
              <td>95.00</td>
              <td>81.33</td>
              <td>78.33</td>
              <td>54.67</td>
              <td>49.33</td>
              <td>6.67</td>
              <td>60.89</td>
              <td>77.02</td>
            </tr>
            <tr>
              <td>Mistral-Nemo</td>
              <td>12B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>61.98</td>
              <td>7.8</td>
              <td>84.74</td>
              <td>85.67</td>
              <td>84.61</td>
              <td>83.67</td>
              <td>84.99</td>
              <td>91.82</td>
              <td>81.80</td>
              <td>71.33</td>
              <td>97.00</td>
              <td>79.00</td>
              <td>77.33</td>
              <td>42.33</td>
              <td>59.67</td>
              <td>7.33</td>
              <td>60.44</td>
              <td>76.02</td>
            </tr>
            <tr>
              <td>Mistral-Nemo</td>
              <td>12B</td>
              <td>FP8</td>
              <td>--</td>
              <td>--</td>
              <td>87.31</td>
              <td>86.58</td>
              <td>85.67</td>
              <td>85.77</td>
              <td>85.29</td>
              <td>92.19</td>
              <td>83.16</td>
              <td>73.41</td>
              <td>95.00</td>
              <td>78.67</td>
              <td>77.33</td>
              <td>50.33</td>
              <td>48.33</td>
              <td>9.00</td>
              <td>59.78</td>
              <td>76.43</td>
            </tr>
            <td>SmolLM2</td>
            <td>1.7B</td>
            <td>None</td>
            <td>6.55</td>
            <td>3.2</td>
            <td>46.17</td>
            <td>43.75</td>
            <td>44.23</td>
            <td>41.47</td>
            <td>44.78</td>
            <td>75.04</td>
            <td>54.21</td>
            <td>53.18</td>
            <td>55.33</td>
            <td>28.00</td>
            <td>14.67</td>
            <td>2.67</td>
            <td>0.33</td>
            <td>0.00</td>
            <td>16.83</td>
            <td>36.99</td>
            </tr>
            <tr>
              <td>Minitron</td>
              <td>4B</td>
              <td>None</td>
              <td>16.01</td>
              <td>7.9</td>
              <td>27.95</td>
              <td>28.68</td>
              <td>35.41</td>
              <td>34.80</td>
              <td>34.07</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>--</td>
              <td>32.18</td>
            </tr>
            <tr>
              <td>Hymba</td>
              <td>1.5B</td>
              <td>None</td>
              <td>--</td>
              <td>2.9</td>
              <td>53.75</td>
              <td>53.53</td>
              <td>52.87</td>
              <td>52.99</td>
              <td>52.74</td>
              <td>84.57</td>
              <td>66.78</td>
              <td>64.73</td>
              <td>34.67</td>
              <td>12.00</td>
              <td>1.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>7.95</td>
              <td>38.10</td>
            </tr>
            <tr>
              <td>Phi-3.5-mini</td>
              <td>3.8B</td>
              <td>None</td>
              <td>14.6</td>
              <td>7.2</td>
              <td>85.47</td>
              <td>87.14</td>
              <td>82.97</td>
              <td>80.74</td>
              <td>82.89</td>
              <td>95.09</td>
              <td>86.89</td>
              <td>76.11</td>
              <td>90.33</td>
              <td>77.33</td>
              <td>68.67</td>
              <td>18.33</td>
              <td>29.00</td>
              <td>0.33</td>
              <td>47.33</td>
              <td>70.44</td>
            </tr>
            <tr>
              <td>Phi-3-small</td>
              <td>7B</td>
              <td>None</td>
              <td>--</td>
              <td>17.95</td>
              <td>70.10</td>
              <td>81.73</td>
              <td>83.14</td>
              <td>86.02</td>
              <td>83.62</td>
              <td>97.12</td>
              <td>91.38</td>
              <td>79.85</td>
              <td>98.00</td>
              <td>93.33</td>
              <td>69.00</td>
              <td>52.00</td>
              <td>9.33</td>
              <td>0.67</td>
              <td>53.72</td>
              <td>74.69</td>
            </tr>
            <!-- Qwen2 0.5B Models -->
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>None</td>
              <td>2.02</td>
              <td>0.95</td>
              <td>37.25</td>
              <td>38.31</td>
              <td>26.38</td>
              <td>28.46</td>
              <td>26.76</td>
              <td>56.41</td>
              <td>40.44</td>
              <td>48.13</td>
              <td>10.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.72</td>
              <td>26.25</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>GPTQ 8-bit</td>
              <td>0.71</td>
              <td>1.4</td>
              <td>38.08</td>
              <td>37.91</td>
              <td>26.33</td>
              <td>27.27</td>
              <td>26.59</td>
              <td>56.13</td>
              <td>40.30</td>
              <td>47.50</td>
              <td>7.67</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.33</td>
              <td>25.84</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>GPTQ 4-bit</td>
              <td>1.12</td>
              <td>0.71</td>
              <td>21.51</td>
              <td>25.32</td>
              <td>14.38</td>
              <td>16.76</td>
              <td>14.23</td>
              <td>52.05</td>
              <td>37.03</td>
              <td>43.11</td>
              <td>2.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.33</td>
              <td>22.64</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>1.38</td>
              <td>0.61</td>
              <td>37.68</td>
              <td>38.13</td>
              <td>26.43</td>
              <td>26.54</td>
              <td>26.81</td>
              <td>56.51</td>
              <td>39.87</td>
              <td>47.23</td>
              <td>11.67</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>2.00</td>
              <td>26.10</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>1.38</td>
              <td>0.87</td>
              <td>37.60</td>
              <td>37.50</td>
              <td>26.23</td>
              <td>26.99</td>
              <td>25.78</td>
              <td>55.36</td>
              <td>40.27</td>
              <td>47.45</td>
              <td>7.33</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.28</td>
              <td>25.35</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>1.51</td>
              <td>0.71</td>
              <td>25.42</td>
              <td>27.32</td>
              <td>18.09</td>
              <td>18.35</td>
              <td>16.40</td>
              <td>50.56</td>
              <td>36.63</td>
              <td>42.42</td>
              <td>5.67</td>
              <td>0.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.06</td>
              <td>24.05</td>
            </tr>
            <tr>
              <td>Qwen2-0.5B</td>
              <td>0.5B</td>
              <td>FP8</td>
              <td>--</td>
              <td>--</td>
              <td>35.20</td>
              <td>35.94</td>
              <td>23.17</td>
              <td>25.25</td>
              <td>22.52</td>
              <td>56.61</td>
              <td>40.13</td>
              <td>46.76</td>
              <td>6.33</td>
              <td>0.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.17</td>
              <td>29.26</td>
            </tr>

            <!-- Qwen2 1.5B Models -->
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>None</td>
              <td>7.09</td>
              <td>2.9</td>
              <td>62.83</td>
              <td>64.85</td>
              <td>56.46</td>
              <td>59.51</td>
              <td>55.88</td>
              <td>84.34</td>
              <td>67.29</td>
              <td>69.78</td>
              <td>44.67</td>
              <td>21.33</td>
              <td>7.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>12.22</td>
              <td>46.19</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>GPTQ 8-bit</td>
              <td>2.54</td>
              <td>3.1</td>
              <td>62.85</td>
              <td>63.86</td>
              <td>57.16</td>
              <td>59.79</td>
              <td>57.24</td>
              <td>84.19</td>
              <td>66.55</td>
              <td>69.97</td>
              <td>46.33</td>
              <td>20.00</td>
              <td>7.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>12.28</td>
              <td>46.42</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>GPTQ 4-bit</td>
              <td>1.81</td>
              <td>2.4</td>
              <td>56.31</td>
              <td>57.54</td>
              <td>49.41</td>
              <td>52.99</td>
              <td>49.66</td>
              <td>82.03</td>
              <td>63.99</td>
              <td>68.99</td>
              <td>33.00</td>
              <td>13.00</td>
              <td>3.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>8.22</td>
              <td>42.00</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>2.51</td>
              <td>1.7</td>
              <td>62.98</td>
              <td>64.04</td>
              <td>56.41</td>
              <td>59.72</td>
              <td>57.19</td>
              <td>83.96</td>
              <td>66.84</td>
              <td>70.19</td>
              <td>47.67</td>
              <td>21.33</td>
              <td>5.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>12.45</td>
              <td>46.67</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>2.48</td>
              <td>2.2</td>
              <td>62.45</td>
              <td>63.00</td>
              <td>54.13</td>
              <td>58.73</td>
              <td>55.75</td>
              <td>83.64</td>
              <td>66.84</td>
              <td>69.72</td>
              <td>46.33</td>
              <td>21.33</td>
              <td>5.67</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>12.22</td>
              <td>46.13</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>3.14</td>
              <td>1.6</td>
              <td>57.90</td>
              <td>58.55</td>
              <td>48.40</td>
              <td>53.10</td>
              <td>48.29</td>
              <td>81.64</td>
              <td>63.51</td>
              <td>66.42</td>
              <td>43.00</td>
              <td>17.67</td>
              <td>6.00</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>11.17</td>
              <td>42.87</td>
            </tr>
            <tr>
              <td>Qwen2-1.5B</td>
              <td>1.5B</td>
              <td>FP8</td>
              <td>--</td>
              <td>--</td>
              <td>61.97</td>
              <td>63.38</td>
              <td>53.88</td>
              <td>57.27</td>
              <td>54.28</td>
              <td>83.77</td>
              <td>66.33</td>
              <td>68.93</td>
              <td>42.33</td>
              <td>19.33</td>
              <td>7.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>11.44</td>
              <td>45.21</td>
            </tr>

            <!-- Qwen2 7B Models -->
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>None</td>
              <td>30.05</td>
              <td>15</td>
              <td>87.14</td>
              <td>87.34</td>
              <td>86.58</td>
              <td>85.82</td>
              <td>86.40</td>
              <td>94.21</td>
              <td>85.52</td>
              <td>80.54</td>
              <td>83.33</td>
              <td>80.33</td>
              <td>45.00</td>
              <td>36.33</td>
              <td>15.00</td>
              <td>2.67</td>
              <td>43.78</td>
              <td>68.35</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>GPTQ 8-bit</td>
              <td>9.63</td>
              <td>8.3</td>
              <td>87.16</td>
              <td>87.54</td>
              <td>86.56</td>
              <td>86.50</td>
              <td>86.40</td>
              <td>94.28</td>
              <td>85.64</td>
              <td>80.04</td>
              <td>84.33</td>
              <td>82.67</td>
              <td>44.00</td>
              <td>32.33</td>
              <td>14.67</td>
              <td>3.00</td>
              <td>43.50</td>
              <td>68.35</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>GPTQ 4-bit</td>
              <td>6.48</td>
              <td>5.3</td>
              <td>85.54</td>
              <td>86.35</td>
              <td>85.92</td>
              <td>84.96</td>
              <td>85.42</td>
              <td>93.45</td>
              <td>85.52</td>
              <td>78.92</td>
              <td>80.67</td>
              <td>72.00</td>
              <td>33.33</td>
              <td>23.33</td>
              <td>4.33</td>
              <td>0.33</td>
              <td>35.67</td>
              <td>64.91</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>W: INT8 &amp; A: INT16</td>
              <td>9.42</td>
              <td>8.2</td>
              <td>86.40</td>
              <td>87.06</td>
              <td>86.15</td>
              <td>85.97</td>
              <td>86.38</td>
              <td>93.91</td>
              <td>85.47</td>
              <td>80.13</td>
              <td>84.00</td>
              <td>80.67</td>
              <td>43.67</td>
              <td>35.00</td>
              <td>14.33</td>
              <td>1.67</td>
              <td>43.22</td>
              <td>67.59</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>W: INT8 &amp; A: INT8</td>
              <td>9.58</td>
              <td>8.2</td>
              <td>87.11</td>
              <td>87.31</td>
              <td>86.63</td>
              <td>86.58</td>
              <td>86.56</td>
              <td>94.02</td>
              <td>85.38</td>
              <td>79.66</td>
              <td>79.33</td>
              <td>83.67</td>
              <td>40.33</td>
              <td>31.67</td>
              <td>17.00</td>
              <td>0.67</td>
              <td>42.11</td>
              <td>67.69</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>W: INT4 &amp; A: INT16</td>
              <td>12.96</td>
              <td>5.3</td>
              <td>84.53</td>
              <td>85.57</td>
              <td>85.32</td>
              <td>84.91</td>
              <td>85.19</td>
              <td>94.22</td>
              <td>84.95</td>
              <td>78.98</td>
              <td>79.67</td>
              <td>77.00</td>
              <td>43.00</td>
              <td>26.67</td>
              <td>5.33</td>
              <td>0.00</td>
              <td>38.61</td>
              <td>65.85</td>
            </tr>
            <tr>
              <td>Qwen2-7B</td>
              <td>7B</td>
              <td>FP8</td>
              <td>--</td>
              <td>--</td>
              <td>86.66</td>
              <td>87.14</td>
              <td>86.05</td>
              <td>86.56</td>
              <td>86.13</td>
              <td>94.26</td>
              <td>85.41</td>
              <td>80.32</td>
              <td>81.00</td>
              <td>83.00</td>
              <td>47.00</td>
              <td>29.00</td>
              <td>13.33</td>
              <td>1.00</td>
              <td>42.39</td>
              <td>67.45</td>
            </tr>
            <!-- Qwen2.5 0.5B Models -->
            <tr>
              <td>Qwen2.5-0.5B</td>
              <td>0.5B</td>
              <td>None</td>
              <td>2.02</td>
              <td>0.95</td>
              <td>46.80</td>
              <td>46.88</td>
              <td>42.73</td>
              <td>43.19</td>
              <td>42.28</td>
              <td>62.50</td>
              <td>44.28</td>
              <td>46.90</td>
              <td>11.67</td>
              <td>3.67</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>2.61</td>
              <td>24.37</td>
            </tr>
            <tr>
              <td>Qwen2.5-0.5B</td>
              <td>0.5B</td>
              <td>GPTQ 8-bit</td>
              <td>0.71</td>
              <td>0.62</td>
              <td>46.85</td>
              <td>47.18</td>
              <td>42.20</td>
              <td>44.20</td>
              <td>42.25</td>
              <td>61.74</td>
              <td>44.43</td>
              <td>46.19</td>
              <td>12.67</td>
              <td>3.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>2.61</td>
              <td>24.23</td>
            </tr>
            <tr>
              <td>Qwen2.5-0.5B</td>
              <td>0.5B</td>
              <td>GPTQ 4-bit</td>
              <td>1.12</td>
              <td>0.45</td>
              <td>34.62</td>
              <td>32.85</td>
              <td>28.15</td>
              <td>27.52</td>
              <td>27.80</td>
              <td>52.58</td>
              <td>37.63</td>
              <td>36.42</td>
              <td>5.33</td>
              <td>3.33</td>
              <td>0.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>1.50</td>
              <td>19.04</td>
            </tr>

            <!-- Qwen2.5 1.5B Models -->
            <tr>
              <td>Qwen2.5-1.5B</td>
              <td>1.5B</td>
              <td>None</td>
              <td>6.68</td>
              <td>2.9</td>
              <td>70.00</td>
              <td>70.20</td>
              <td>69.72</td>
              <td>68.46</td>
              <td>69.90</td>
              <td>87.58</td>
              <td>73.81</td>
              <td>71.85</td>
              <td>66.33</td>
              <td>65.33</td>
              <td>34.33</td>
              <td>7.33</td>
              <td>1.33</td>
              <td>0.00</td>
              <td>29.11</td>
              <td>53.22</td>
            </tr>
            <tr>
              <td>Qwen2.5-1.5B</td>
              <td>1.5B</td>
              <td>GPTQ 8-bit</td>
              <td>2.54</td>
              <td>1.7</td>
              <td>70.33</td>
              <td>70.33</td>
              <td>70.03</td>
              <td>68.99</td>
              <td>69.52</td>
              <td>87.78</td>
              <td>73.72</td>
              <td>72.10</td>
              <td>68.33</td>
              <td>65.33</td>
              <td>36.67</td>
              <td>8.00</td>
              <td>1.33</td>
              <td>0.00</td>
              <td>29.94</td>
              <td>54.10</td>
            </tr>
            <tr>
              <td>Qwen2.5-1.5B</td>
              <td>1.5B</td>
              <td>GPTQ 4-bit</td>
              <td>1.81</td>
              <td>1.1</td>
              <td>64.92</td>
              <td>64.92</td>
              <td>62.40</td>
              <td>63.28</td>
              <td>62.42</td>
              <td>86.25</td>
              <td>70.25</td>
              <td>69.10</td>
              <td>60.00</td>
              <td>46.67</td>
              <td>12.67</td>
              <td>7.33</td>
              <td>0.00</td>
              <td>0.00</td>
              <td>21.11</td>
              <td>47.02</td>
            </tr>

            <!-- Qwen2.5 3B Models -->
            <tr>
              <td>Qwen2.5-3B</td>
              <td>3B</td>
              <td>None</td>
              <td>12.42</td>
              <td>5.8</td>
              <td>84.74</td>
              <td>84.38</td>
              <td>85.44</td>
              <td>84.96</td>
              <td>85.44</td>
              <td>93.49</td>
              <td>83.73</td>
              <td>76.25</td>
              <td>78.33</td>
              <td>75.33</td>
              <td>47.67</td>
              <td>34.33</td>
              <td>2.67</td>
              <td>1.00</td>
              <td>39.89</td>
              <td>65.78</td>
            </tr>
            <tr>
              <td>Qwen2.5-3B</td>
              <td>3B</td>
              <td>GPTQ 8-bit</td>
              <td>4.21</td>
              <td>3.3</td>
              <td>85.17</td>
              <td>84.99</td>
              <td>84.38</td>
              <td>84.38</td>
              <td>84.71</td>
              <td>93.55</td>
              <td>83.53</td>
              <td>76.77</td>
              <td>80.33</td>
              <td>75.00</td>
              <td>47.67</td>
              <td>32.67</td>
              <td>2.00</td>
              <td>1.00</td>
              <td>39.78</td>
              <td>65.87</td>
            </tr>
            <tr>
              <td>Qwen2.5-3B</td>
              <td>3B</td>
              <td>GPTQ 4-bit</td>
              <td>2.88</td>
              <td>2.0</td>
              <td>81.78</td>
              <td>81.60</td>
              <td>81.58</td>
              <td>81.91</td>
              <td>81.78</td>
              <td>92.12</td>
              <td>80.86</td>
              <td>71.96</td>
              <td>72.67</td>
              <td>65.67</td>
              <td>17.67</td>
              <td>19.67</td>
              <td>0.00</td>
              <td>1.00</td>
              <td>29.45</td>
              <td>61.29</td>
            </tr>

            <!-- Qwen2.5 7B Models -->
            <tr>
              <td>Qwen2.5-7B</td>
              <td>7B</td>
              <td>None</td>
              <td>30.05</td>
              <td>15</td>
              <td>91.76</td>
              <td>92.19</td>
              <td>91.05</td>
              <td>91.89</td>
              <td>91.33</td>
              <td>96.03</td>
              <td>90.53</td>
              <td>82.66</td>
              <td>94.33</td>
              <td>90.00</td>
              <td>69.67</td>
              <td>47.00</td>
              <td>39.33</td>
              <td>5.67</td>
              <td>57.67</td>
              <td>78.36</td>
            </tr>
            <tr>
              <td>Qwen2.5-7B</td>
              <td>7B</td>
              <td>GPTQ 8-bit</td>
              <td>9.63</td>
              <td>8.3</td>
              <td>91.84</td>
              <td>92.22</td>
              <td>91.81</td>
              <td>91.56</td>
              <td>91.31</td>
              <td>96.03</td>
              <td>90.64</td>
              <td>82.58</td>
              <td>94.00</td>
              <td>92.00</td>
              <td>71.33</td>
              <td>49.00</td>
              <td>41.33</td>
              <td>5.67</td>
              <td>58.89</td>
              <td>78.87</td>
            </tr>
            <tr>
              <td>Qwen2.5-7B</td>
              <td>7B</td>
              <td>GPTQ 4-bit</td>
              <td>6.48</td>
              <td>5.3</td>
              <td>90.62</td>
              <td>91.23</td>
              <td>90.65</td>
              <td>90.73</td>
              <td>90.85</td>
              <td>95.62</td>
              <td>89.19</td>
              <td>82.69</td>
              <td>80.67</td>
              <td>15.00</td>
              <td>58.33</td>
              <td>15.67</td>
              <td>31.67</td>
              <td>1.00</td>
              <td>33.72</td>
              <td>68.35</td>
            </tr>

            <!-- Qwen2.5 14B Models -->
            <tr>
              <td>Qwen2.5-14B</td>
              <td>14B</td>
              <td>None</td>
              <td>57.04</td>
              <td>28</td>
              <td>94.29</td>
              <td>94.57</td>
              <td>94.06</td>
              <td>94.54</td>
              <td>93.86</td>
              <td>97.87</td>
              <td>93.37</td>
              <td>84.08</td>
              <td>96.33</td>
              <td>95.33</td>
              <td>84.00</td>
              <td>72.00</td>
              <td>61.33</td>
              <td>38.67</td>
              <td>74.61</td>
              <td>85.80</td>
            </tr>
            <tr>
              <td>Qwen2.5-14B</td>
              <td>14B</td>
              <td>GPTQ 8-bit</td>
              <td>17.24</td>
              <td>16</td>
              <td>94.49</td>
              <td>94.95</td>
              <td>93.71</td>
              <td>94.59</td>
              <td>94.11</td>
              <td>97.90</td>
              <td>93.71</td>
              <td>84.22</td>
              <td>96.33</td>
              <td>95.00</td>
              <td>84.00</td>
              <td>72.00</td>
              <td>65.00</td>
              <td>36.33</td>
              <td>74.78</td>
              <td>86.02</td>
            </tr>
            <tr>
              <td>Qwen2.5-14B</td>
              <td>14B</td>
              <td>GPTQ 4-bit</td>
              <td>10.65</td>
              <td>9.4</td>
              <td>94.74</td>
              <td>94.69</td>
              <td>94.01</td>
              <td>94.31</td>
              <td>93.63</td>
              <td>97.57</td>
              <td>93.17</td>
              <td>83.10</td>
              <td>95.00</td>
              <td>95.67</td>
              <td>82.33</td>
              <td>64.00</td>
              <td>54.33</td>
              <td>26.00</td>
              <td>69.56</td>
              <td>83.53</td>
            </tr>

            <!-- Qwen2.5 32B Models -->
            <tr>
              <td>Qwen2.5-32B</td>
              <td>32B</td>
              <td>None</td>
              <td>125</td>
              <td>62</td>
              <td>95.40</td>
              <td>95.78</td>
              <td>95.20</td>
              <td>95.55</td>
              <td>94.92</td>
              <td>98.26</td>
              <td>95.25</td>
              <td>87.11</td>
              <td>99.00</td>
              <td>99.33</td>
              <td>93.33</td>
              <td>92.33</td>
              <td>79.00</td>
              <td>60.00</td>
              <td>87.17</td>
              <td>91.77</td>
            </tr>
            <tr>
              <td>Qwen2.5-32B</td>
              <td>32B</td>
              <td>GPTQ 8-bit</td>
              <td>33.81</td>
              <td>33</td>
              <td>95.73</td>
              <td>95.86</td>
              <td>95.50</td>
              <td>95.60</td>
              <td>95.25</td>
              <td>98.34</td>
              <td>95.16</td>
              <td>86.62</td>
              <td>99.00</td>
              <td>99.00</td>
              <td>93.33</td>
              <td>92.33</td>
              <td>79.67</td>
              <td>61.00</td>
              <td>87.39</td>
              <td>91.86</td>
            </tr>
            <tr>
              <td>Qwen2.5-32B</td>
              <td>32B</td>
              <td>GPTQ 4-bit</td>
              <td>52.42</td>
              <td>19</td>
              <td>95.73</td>
              <td>95.73</td>
              <td>94.92</td>
              <td>95.43</td>
              <td>95.12</td>
              <td>98.09</td>
              <td>95.19</td>
              <td>87.06</td>
              <td>100.00</td>
              <td>100.00</td>
              <td>98.33</td>
              <td>91.67</td>
              <td>77.33</td>
              <td>56.33</td>
              <td>87.28</td>
              <td>91.80</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p class="has-text-centered mt-2">
        <small>Note: The sorting tasks involve arranging lists of numbers, with (+ve) containing only positive numbers
          and (mixed) containing both positive and negative numbers.</small>
      </p>
    </div>
  </section>

  <!-- Key findings section -->
  <section class="section hero is-light">
    <div class="container is-max-widescreen">
      <h2 class="title is-3 has-text-centered">Key Findings</h2>
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">

            <!-- 1 -->
            <div class="box">
              <h4 class="title is-4">Small ≠ Weak — 32 B Qwen rivals GPT‑4 Turbo!</h4>
              <p>Qwen 2.5‑32B matches GPT‑4‑Turbo on intermediate‑reasoning (MR‑GSM8K 55.6 vs 53.0) and reaches 95 % on
                GSM8K while using&nbsp;≈1⁄5 the parameters, overturning the “> 100 B for reasoning” myth.</p>
            </div>

            <!-- 2 -->
            <div class="box">
              <h4 class="title is-4">Quantization is (almost) free!</h4>
              <p>4‑ to 8‑bit GPTQ cuts GPU memory by up to 75 % yet preserves ≥ 99 % of accuracy across GSM8K, ARC, CQA
                and robustness benchmarks, enabling laptop‑scale deployment of formerly heavyweight models. </p>
            </div>

            <!-- 4 -->
            <div class="box">
              <h4 class="title is-4">Keep prompts simple!</h4>
              <p>On GSM8K, direct I/O prompts outperform or equal Chain‑of‑Thought and multi‑shot variants; additional
                “think‑step” instructions often confuse SLMs instead of helping them.</p>
            </div>

            <!-- 5 -->
            <div class="box">
              <h4 class="title is-4">Sequence length is the Achilles’ heel!</h4>
              <p>Accuracy on sorting jumps from > 80 % (8‑item lists) to &lt; 40 % (32‑item mixed lists), and negative
                numbers exacerbate errors, revealing a context‑length bottleneck for algorithmic reasoning.</p>
            </div>

            <!-- 6 -->
            <div class="box">
              <h4 class="title is-4">Robustness scales with size—but survives quantization!</h4>
              <p>Larger SLMs (32B, 70B) remain the most resilient to adversarial GSM‑Plus inputs, yet their quantized
                versions show negligible degradation, whereas pruned counterparts collapse.</p>
            </div>

            <!-- 3 -->
            <div class="box">
              <h4 class="title is-4">Pruning hurts, sometimes fatally!</h4>
              <p>Weight‑pruned 8 B models lose 30–50 points on reasoning tasks and score ~0 on MR‑GSM8K, showing that
                aggressive sparsification cripples logical consistency even after recovery fine‑tuning.</p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- BibTeX citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{srivastava2025towards,
  title={Towards reasoning ability of small language models},
  author={Srivastava, Gaurav and Cao, Shuxiang and Wang, Xuan},
  journal={arXiv preprint arXiv:2502.11569},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>